{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985f2fa-b620-4a68-a26d-b4e5ec3af592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Generic helpers ----------\n",
    "def _to_dt(s, dayfirst=False):\n",
    "    \"\"\"Safe datetime parser (returns NaT on bad rows).\"\"\"\n",
    "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst)\n",
    "\n",
    "def _to_binary_fraud(s):\n",
    "    \"\"\"Map common fraud flags to {0,1}. Unmapped -> NaN.\"\"\"\n",
    "    m = {\"y\":1, \"yes\":1, \"true\":1, \"t\":1, \"1\":1,\n",
    "         \"n\":0, \"no\":0, \"false\":0, \"f\":0, \"0\":0}\n",
    "    return s.astype(str).str.strip().str.lower().map(m)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Temporal features from incident_date (and optional time)\n",
    "# ============================================================\n",
    "def add_incident_month(df, date_col=\"incident_date\"):\n",
    "    out = df.copy()\n",
    "    dt = _to_dt(out[date_col])\n",
    "    out[\"incident_month\"] = dt.dt.month\n",
    "    return out\n",
    "\n",
    "def add_incident_dayofweek(df, date_col=\"incident_date\"):\n",
    "    out = df.copy()\n",
    "    dt = _to_dt(out[date_col])\n",
    "    # Monday=0 ... Sunday=6\n",
    "    out[\"incident_dayofweek\"] = dt.dt.dayofweek\n",
    "    return out\n",
    "\n",
    "def add_incident_hour(df, time_col=\"incident_hour_of_the_day\"):\n",
    "    \"\"\"\n",
    "    If you have a dedicated integer hour column use that directly.\n",
    "    If you have a string time like '14:35', the parser below tries to extract the hour.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    s = out[time_col]\n",
    "    # Try to coerce strings like '14', '14:35', '14.0'\n",
    "    hour = pd.to_numeric(s, errors=\"coerce\")\n",
    "    needs_parse = hour.isna()\n",
    "    if needs_parse.any():\n",
    "        # string parse fallback\n",
    "        parsed = pd.to_datetime(s[needs_parse], errors=\"coerce\", format=\"%H:%M\")\n",
    "        hour.loc[needs_parse] = parsed.dt.hour\n",
    "    out[\"incident_hour\"] = hour.astype(\"Int64\")\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 2) days_to_report = police_report_date - incident_date\n",
    "# ============================================================\n",
    "def add_days_to_report(df, incident_col=\"incident_date\", report_col=\"police_report_date\"):\n",
    "    out = df.copy()\n",
    "    inc = _to_dt(out[incident_col])\n",
    "    rep = _to_dt(out[report_col])\n",
    "    out[\"days_to_report\"] = (rep - inc).dt.days\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 3) policy_tenure (years from bind date to incident)\n",
    "# ============================================================\n",
    "def add_policy_tenure_years(df, bind_col=\"policy_bind_date\", incident_col=\"incident_date\"):\n",
    "    out = df.copy()\n",
    "    bind = _to_dt(out[bind_col])\n",
    "    inc  = _to_dt(out[incident_col])\n",
    "    # Use 365.25 to account for leap years\n",
    "    out[\"policy_tenure_years\"] = ((inc - bind).dt.days / 365.25).round(3)\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 4) policy_deductible_ratio = deductible / annual_premium\n",
    "# ============================================================\n",
    "def add_policy_deductible_ratio(df, ded_col=\"policy_deductable\", prem_col=\"policy_annual_premium\"):\n",
    "    out = df.copy()\n",
    "    denom = pd.to_numeric(out[prem_col], errors=\"coerce\").replace({0: np.nan})\n",
    "    numer = pd.to_numeric(out[ded_col], errors=\"coerce\")\n",
    "    out[\"policy_deductible_ratio\"] = numer / denom\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 5) vehicle_age = current_year - auto_year\n",
    "# ============================================================\n",
    "def add_vehicle_age(df, auto_year_col=\"auto_year\", current_year=None):\n",
    "    out = df.copy()\n",
    "    year = pd.to_numeric(out[auto_year_col], errors=\"coerce\")\n",
    "    if current_year is None:\n",
    "        # If incident year exists, prefer that; else use today's year\n",
    "        inc_dt = _to_dt(out.get(\"incident_date\"))\n",
    "        current_year = int(inc_dt.dt.year.mode().iloc[0]) if \"incident_date\" in out and inc_dt.notna().any() \\\n",
    "                       else pd.Timestamp.today().year\n",
    "    out[\"vehicle_age\"] = current_year - year\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 6) total_claim_amount = injury + property + vehicle (robust)\n",
    "# ============================================================\n",
    "def add_total_claim_amount(df, injury_col=\"injury_claim\", property_col=\"property_claim\", vehicle_col=\"vehicle_claim\"):\n",
    "    out = df.copy()\n",
    "    i = pd.to_numeric(out[injury_col], errors=\"coerce\").fillna(0)\n",
    "    p = pd.to_numeric(out[property_col], errors=\"coerce\").fillna(0)\n",
    "    v = pd.to_numeric(out[vehicle_col], errors=\"coerce\").fillna(0)\n",
    "    out[\"total_claim_amount\"] = i + p + v\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 7) high_claim_flag (>= 90th percentile of total_claim_amount)\n",
    "# ============================================================\n",
    "def add_high_claim_flag(df, total_col=\"total_claim_amount\", q=90):\n",
    "    out = df.copy()\n",
    "    vals = pd.to_numeric(out[total_col], errors=\"coerce\")\n",
    "    thresh = np.nanpercentile(vals, q) if np.isfinite(vals).any() else np.nan\n",
    "    out[\"high_claim_flag\"] = (vals >= thresh).astype(\"Int64\")\n",
    "    out.attrs[\"high_claim_threshold\"] = thresh  # stored as metadata for reference\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 8) Geographic: mismatch + frequency risk by state\n",
    "# ============================================================\n",
    "def add_state_mismatch(df, insured_state_col=\"policy_state\", incident_state_col=\"incident_state\"):\n",
    "    out = df.copy()\n",
    "    out[\"insured_vs_incident_state_mismatch\"] = (\n",
    "        out[insured_state_col].astype(str).str.upper()\n",
    "        != out[incident_state_col].astype(str).str.upper()\n",
    "    ).astype(\"Int64\")\n",
    "    return out\n",
    "\n",
    "def add_incident_state_risk(df, state_col=\"incident_state\", fraud_col=\"fraud_reported\"):\n",
    "    \"\"\"\n",
    "    Encodes per-state fraud propensity: mean(fraud) within each state.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    y = _to_binary_fraud(out[fraud_col])\n",
    "    grp = y.groupby(out[state_col]).transform(\"mean\")\n",
    "    out[\"incident_state_risk\"] = grp\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 9) Employment/occupation fraud rate (frequency encoding)\n",
    "# ============================================================\n",
    "def add_fraud_rate_by_occupation(df, occ_col=\"insured_occupation\", fraud_col=\"fraud_reported\", min_count=10):\n",
    "    out = df.copy()\n",
    "    y = _to_binary_fraud(out[fraud_col])\n",
    "    grp_mean = y.groupby(out[occ_col]).transform(\"mean\")\n",
    "    grp_cnt  = out.groupby(occ_col)[occ_col].transform(\"count\")\n",
    "    # Optional smoothing: fallback to global mean when category too small\n",
    "    global_mean = y.mean()\n",
    "    smoothed = np.where(grp_cnt >= min_count, grp_mean, global_mean)\n",
    "    out[\"fraud_rate_by_occupation\"] = smoothed\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 10) Multiple claims flag per insured (repeat customer behavior)\n",
    "# ============================================================\n",
    "def add_multiple_claims_flag(df, id_col=\"policy_number\"):\n",
    "    \"\"\"\n",
    "    Set 1 if this policy_number appears >1 times in the dataset.\n",
    "    If you have a better unique key for the claimant, replace policy_number with that.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    counts = out.groupby(id_col)[id_col].transform(\"count\")\n",
    "    out[\"multiple_claims_flag\"] = (counts > 1).astype(\"Int64\")\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
